import numpy as np
from tensorflow.keras import Sequential
from tensorflow.keras.layers import Embedding, LSTM, Dense
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from sklearn.model_selection import train_test_split

# =============================
# 1. å°å‹è®­ç»ƒæ•°æ®æ ·æœ¬
# =============================
items = [
    "å¡‘æ–™ç“¶", "çŸ¿æ³‰æ°´ç“¶", "å•¤é…’ç“¶", "é¥®æ–™ç“¶", "æ˜“æ‹‰ç½", "å¿«é€’çº¸ç›’", "ç»ç’ƒç“¶",
    "æœçš®", "å‰©é¥­", "èœå¶", "èŒ¶å¶æ¸£", "éª¨å¤´", "è¥¿ç“œçš®", "é¦™è•‰çš®",
    "ç”µæ± ", "è§å…‰ç¯", "è¯å“", "æ²¹æ¼†æ¡¶", "åºŸæ—§ç¯æ³¡",
    "ç°åœŸ", "çº¸å·¾", "çƒŸå¤´", "å°˜åœŸ", "å£ç½©"
]

labels = [
    0,0,0,0,0,0,0,   # å¯å›æ”¶ç‰© 0
    1,1,1,1,1,1,1,   # å¨ä½™åƒåœ¾ 1
    2,2,2,2,2,       # æœ‰å®³åƒåœ¾ 2
    3,3,3,3,3        # å…¶ä»–åƒåœ¾ 3
]

label_names = ["å¯å›æ”¶ç‰©â™»", "å¨ä½™åƒåœ¾ğŸƒ", "æœ‰å®³åƒåœ¾â˜£", "å…¶ä»–åƒåœ¾ğŸ—‘"]

# =============================
# 2. Text Tokenize
# =============================
tokenizer = Tokenizer()
tokenizer.fit_on_texts(items)

X = tokenizer.texts_to_sequences(items)
X = pad_sequences(X, maxlen=3)
y = np.array(labels)

# =============================
# 3. Build & Train model
# =============================
model = Sequential([
    Embedding(input_dim=50, output_dim=16, input_length=3),
    LSTM(32),
    Dense(4, activation="softmax")
])

model.compile(optimizer="adam", loss="sparse_categorical_crossentropy", metrics=["accuracy"])
model.fit(X, y, epochs=50, verbose=0)

print("è®­ç»ƒå®Œæˆï¼")

# =============================
# 4. é¢„æµ‹å‡½æ•°
# =============================
def classify(item):
    seq = tokenizer.texts_to_sequences([item])
    seq = pad_sequences(seq, maxlen=3)
    pred = model.predict(seq)[0]
    return label_names[np.argmax(pred)]

# æµ‹è¯•
print(classify("çŸ¿æ³‰æ°´ç“¶"))
print(classify("å‰©é¥­"))
print(classify("ç”µæ± "))
print(classify("å£ç½©"))
